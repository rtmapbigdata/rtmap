a1.sources = source1
a1.sinks = sink1
a1.channels = channel1

a1.sources.source1.type = cn.rtmap.flume.source.SQLSource
a1.sources.source1.zookeeper.hosts = r1s4:2181,r1s5:2181,r2s5:2181
a1.sources.source1.zookeeper.timeout = 15000
a1.sources.source1.zookeeper.znode.path=/flgt

a1.sources.source1.run.query.delay=30000
a1.sources.source1.status.file.path = /mnt/bigpfs/bcia-queue/conf
a1.sources.source1.status.file.name = sql-flgt.ctl
a1.sources.source1.batch.size = 1000
a1.sources.source1.max.rows = 10000

a1.sources.source1.connection.url = jdbc:oracle:thin:@10.66.2.101:1521:ora10g
#a1.sources.source1.user = bigdata
#a1.sources.source1.password = ypgcwrs

a1.sources.source1.record.count.file.path = /mnt/bigpfs/bcia-queue/qa/flgt
a1.sources.source1.record.count.file.prefix = flgt_

a1.sources.source1.table = v_lk_flgt
a1.sources.source1.check.column.name = OPERATION_DATE
a1.sources.source1.check.column.initial.value = 20151125

a1.sources.source1.last.value.query = select to_char(to_date('%s','yyyymmdd') + 1, 'yyyymmdd') as operation_date from dual
a1.sources.source1.custom.query = select operation_date, trim(flgt_no) flgt_no, trim(actype_code) actype_code, trim(flgttype_code) flgttype_code, trim(d_or_i) d_or_i, trim(airline_code) airline_code, trim(trml_code) trml_code, act_dttm, sch_dttm, trim(stnd_code) stnd_code, trim(a_or_d) a_or_d, ogat_dttm, cgat_dttm, max_pax, guonei, guoji, trim(abnormal_status) abnormal_status, trim(csh_flgtno) csh_flgtno, trim(larpt_code) larpt_code, trim(rout) rout, trim(rout_name) rout_name, lsch_dttm, trim(gate_code) gate_code from ODS.v_lk_flgt where (to_date(operation_date,'yyyymmdd') > to_date('%s','yyyymmdd') and to_date(operation_date,'yyyymmdd') <= to_date('%s','yyyymmdd')) and to_date(operation_date,'yyyymmdd') < to_date(to_char(sysdate,'yyyymmdd'), 'yyyymmdd')

#a1.sources.source1.interceptors = i1
#a1.sources.source1.interceptors.i1.type = cn.rtmap.flume.validation.Validator$Builder

a1.sources.source1.channels = channel1
a1.channels.channel1.type = memory
a1.channels.channel1.capacity = 100000
a1.channels.channel1.transactionCapacity = 100000

# kafka sink
# a1.sinks.sink1.type = com.thilinamb.flume.sink.KafkaSink
# a1.sinks.sink1.custom-topic = lkxxb
# a1.sinks.sink1.preprocessor = com.thilinamb.flume.sink.example.SimpleMessagePreprocessor
# a1.sinks.sink1.kafka.metadata.broker.list = r1s2:6667,r2s2:6667,r2s3:6667,r2s4:6667
# a1.sinks.sink1.kafka.serializer.class = kafka.serializer.StringEncoder
# a1.sinks.sink1.request.required.acks = 1
# a1.sinks.sink1.channel = channel1

a1.sinks.sink1.type = hdfs
a1.sinks.sink1.hdfs.path = /electrocar/flight/%{index}
a1.sinks.sink1.hdfs.fileType = DataStream
a1.sinks.sink1.hdfs.filePrefix = v_lk_flgt
#a1.sinks.sink1.hdfs.inUsePrefix = _
a1.sinks.sink1.hdfs.inUseSuffix = .txt
a1.sinks.sink1.hdfs.rollInterval = 0
a1.sinks.sink1.hdfs.rollSize = 0
a1.sinks.sink1.hdfs.rollCount = 0

#a1.sinks.sink1.type = logger

a1.sinks.sink1.channel = channel1

# job schedule policy
a1.sources.source1.job.schedule.job.key = job1
a1.sources.source1.job.schedule.trigger.key = trigger1
a1.sources.source1.job.schedule.corn.express = */30 * * * * ? *

