a1.sources = r1
a1.sinks = k1
a1.channels = c1

a1.sources.r1.type = cn.rtmap.bigdata.ingest.source.FileSource
a1.sources.r1.schedule.express = */30 * * * * ? *
a1.sources.r1.verf.extension = .verf
a1.sources.r1.data.from = lbs
a1.sources.r1.incoming.dir = /mnt/data/share/ingest/incoming/lbs
a1.sources.r1.backup.dir = /mnt/data/share/ingest/bak/lbs
a1.sources.r1.channels = c1

a1.sources.r1.interceptors = i1 i2 i3
a1.sources.r1.interceptors.i1.type = cn.rtmap.bigdata.ingest.intercept.BatchPackager$Builder
a1.sources.r1.interceptors.i2.type = cn.rtmap.bigdata.ingest.intercept.EventCompressor$Builder
a1.sources.r1.interceptors.i3.type = cn.rtmap.bigdata.ingest.intercept.EventSerializer$Builder
# a1.sources.r1.interceptors.i4.type = cn.rtmap.bigdata.ingest.intercept.EventDeserializer$Builder
# a1.sources.r1.interceptors.i5.type = cn.rtmap.bigdata.ingest.intercept.EventDecompressor$Builder

a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.topic = test1
a1.sinks.k1.brokerList = 211.157.165.91:6668,211.157.165.92:6668,211.157.165.93:6668
a1.sinks.k1.requiredAcks = 1
a1.sinks.k1.batchSize = 20
a1.sinks.k1.channel = c1

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 100000
a1.channels.c1.transactionCapacity = 80000
