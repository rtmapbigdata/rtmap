a1.sources = r1
a1.sinks = k1
a1.channels = c1

a1.sources.r1.type = cn.rtmap.bigdata.ingest.source.FileSource
a1.sources.r1.verf.extension = .verf
a1.sources.r1.incoming.dir = /mnt/data/share/ingest/incoming/lbs
#a1.sources.r1.outgoing.dir = /mnt/data/share/ingest/outgoing/lbs
a1.sources.r1.backup.dir = /mnt/data/share/ingest/bak/lbs
a1.sources.r1.channels = c1

a1.sources.r1.interceptors = i1 i2 i4 i5 i3
#a1.sources.r1.interceptors = i1 i4 i5
a1.sources.r1.interceptors.i1.type = cn.rtmap.bigdata.ingest.intercept.BatchPackager$Builder
a1.sources.r1.interceptors.i2.type = cn.rtmap.bigdata.ingest.intercept.EventCompressor$Builder
a1.sources.r1.interceptors.i4.type = cn.rtmap.bigdata.ingest.intercept.EventSerializer$Builder
a1.sources.r1.interceptors.i5.type = cn.rtmap.bigdata.ingest.intercept.EventDeserializer$Builder
a1.sources.r1.interceptors.i3.type = cn.rtmap.bigdata.ingest.intercept.EventDecompressor$Builder

# Describe the sink
# a1.sinks.k1.type = logger
# a1.sinks.k1.channel = c1

a1.sinks.k1.type = file_roll
a1.sinks.k1.sink.directory = /home/rtmap/test

# a1.sinks.k1.serializer = cn.rtmap.bigdata.ingest.serialization.JSONEventSerializer$Builder
# a1.sinks.k1.serializer.columns = unit_code process_date filename batch_size body
# a1.sinks.k1.serializer.body = body
# a1.sinks.k1.serializer.appendNewline = true

a1.sinks.k1.channel = c1



# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 100000
a1.channels.c1.transactionCapacity = 80000
