a1.sources = r1 r2 r3 r4
a1.sinks = k1 k2 k3 k4
a1.channels = c1 c2 c3 c4

# -------------------------- lbs file --------------------------
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.channels = c1
a1.sources.r1.zookeeperConnect = 192.168.14.44:2181,192.168.14.71:2181,192.168.14.43:2181,192.168.14.120:2181,192.168.14.68:2181
a1.sources.r1.topic = lbs-static
a1.sources.r1.groupId = flume
a1.sources.r1.kafka.consumer.timeout.ms = 100
a1.sources.r1.kafka.fetch.message.max.bytes = 15728640

a1.sources.r1.interceptors = i1 i2
a1.sources.r1.interceptors.i1.type = cn.rtmap.bigdata.ingest.intercept.EventDeserializer$Builder
a1.sources.r1.interceptors.i2.type = cn.rtmap.bigdata.ingest.intercept.EventDecompressor$Builder

# a1.sinks.k1.type = logger
# a1.sinks.k1.channel = c1

a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = /databank/ods/%{from}/%{unit_code}/%{process_month}/%{process_date}
a1.sinks.k1.hdfs.filePrefix = %{filename}
a1.sinks.k1.hdfs.inUsePrefix = _
a1.sinks.k1.hdfs.rollInterval = 0
a1.sinks.k1.hdfs.rollSize = 0
a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.idleTimeout = 300
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzop
a1.sinks.k1.hdfs.callTimeout = 300000

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 100000
a1.channels.c1.transactionCapacity = 10000


# -------------------------- lbs udp --------------------------
a1.sources.r2.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r2.channels = c2
a1.sources.r2.zookeeperConnect = 192.168.14.44:2181,192.168.14.71:2181,192.168.14.43:2181,192.168.14.120:2181,192.168.14.68:2181
a1.sources.r2.topic = lbs-online
a1.sources.r2.groupId = flume
a1.sources.r2.kafka.consumer.timeout.ms = 100
a1.sources.r2.kafka.fetch.message.max.bytes = 15728640

a1.sources.r2.interceptors = i1
a1.sources.r2.interceptors.i1.type = cn.rtmap.bigdata.ingest.intercept.EventDeserializer$Builder

# a1.sinks.k2.type = logger
# a1.sinks.k2.channel = c2

a1.sinks.k2.type = hdfs
a1.sinks.k2.channel = c2
a1.sinks.k2.hdfs.path = /databank/ods/%{from}/%{unit_code}/%{process_month}/%{process_date}
a1.sinks.k2.hdfs.filePrefix = a_%Y%m%d%H%M_%{unit_code}
a1.sinks.k2.hdfs.inUsePrefix = _
a1.sinks.k2.hdfs.round = true
a1.sinks.k2.hdfs.roundValue = 10
a1.sinks.k2.hdfs.roundUnit = minute
a1.sinks.k2.hdfs.fileType = CompressedStream
a1.sinks.k2.hdfs.codeC = lzop

# Use a channel which buffers events in memory
a1.channels.c2.type = memory
a1.channels.c2.capacity = 100000
a1.channels.c2.transactionCapacity = 10000

# -------------------------- market file --------------------------
a1.sources.r3.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r3.channels = c3
a1.sources.r3.zookeeperConnect = 192.168.14.44:2181,192.168.14.71:2181,192.168.14.43:2181,192.168.14.120:2181,192.168.14.68:2181
a1.sources.r3.topic = market
a1.sources.r3.groupId = flume
a1.sources.r3.kafka.consumer.timeout.ms = 100
a1.sources.r3.kafka.fetch.message.max.bytes = 15728640

a1.sources.r3.interceptors = i1 i2
a1.sources.r3.interceptors.i1.type = cn.rtmap.bigdata.ingest.intercept.EventDeserializer$Builder
a1.sources.r3.interceptors.i2.type = cn.rtmap.bigdata.ingest.intercept.EventDecompressor$Builder

# a1.sinks.k3.type = logger
# a1.sinks.k3.channel = c3

a1.sinks.k3.type = hdfs
a1.sinks.k3.channel = c3
a1.sinks.k3.hdfs.path = /databank/ods/%{from}/%{unit_code}/%{process_month}/%{process_date}
a1.sinks.k3.hdfs.filePrefix = %{filename}
a1.sinks.k3.hdfs.inUsePrefix = _
a1.sinks.k3.hdfs.rollInterval = 0
a1.sinks.k3.hdfs.rollSize = 0
a1.sinks.k3.hdfs.rollCount = 0
a1.sinks.k3.hdfs.idleTimeout = 300
a1.sinks.k3.hdfs.fileType = CompressedStream
a1.sinks.k3.hdfs.codeC = lzop
a1.sinks.k3.hdfs.callTimeout = 300000

# Use a channel which buffers events in memory
a1.channels.c3.type = memory
a1.channels.c3.capacity = 100000
a1.channels.c3.transactionCapacity = 10000

# -------------------------- weixin --------------------------
a1.sources.r4.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r4.channels = c4
a1.sources.r4.zookeeperConnect = 192.168.14.44:2181,192.168.14.71:2181,192.168.14.43:2181,192.168.14.120:2181,192.168.14.68:2181
a1.sources.r4.topic = weixin
a1.sources.r4.groupId = flume
a1.sources.r4.kafka.consumer.timeout.ms = 100
a1.sources.r4.kafka.fetch.message.max.bytes = 15728640

a1.sources.r4.interceptors = i1 i2
a1.sources.r4.interceptors.i1.type = cn.rtmap.bigdata.ingest.intercept.EventDeserializer$Builder
a1.sources.r4.interceptors.i2.type = cn.rtmap.bigdata.ingest.intercept.EventDecompressor$Builder

# a1.sinks.k4.type = logger
# a1.sinks.k4.channel = c4

a1.sinks.k4.type = hdfs
a1.sinks.k4.channel = c4
a1.sinks.k4.hdfs.path = /databank/ods/%{from}/%{unit_code}/%{process_month}/%{process_date}
a1.sinks.k4.hdfs.filePrefix = %{filename}
a1.sinks.k4.hdfs.inUsePrefix = _
a1.sinks.k4.hdfs.rollInterval = 0
a1.sinks.k4.hdfs.rollSize = 0
a1.sinks.k4.hdfs.rollCount = 0
a1.sinks.k4.hdfs.idleTimeout = 300
a1.sinks.k4.hdfs.fileType = CompressedStream
a1.sinks.k4.hdfs.codeC = lzop
a1.sinks.k4.hdfs.callTimeout = 300000

# Use a channel which buffers events in memory
a1.channels.c4.type = memory
a1.channels.c4.capacity = 100000
a1.channels.c4.transactionCapacity = 10000
